#!/usr/bin/env bash
# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
# 
# banzai_2019v1.2.sh
# 
# This script is customized to run the MBARI,STANFORD,USF sequences
# updated Sep 2017
# R. Michisaki MBARI
# CUTADAPT. Major reconstruction to run cutadapt for MBARI samples. Add param INDEXED. Put primer removal, cutadapt
# in library level loop. 
# Aug 2019 rpm
# :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

# Pipeline for analysis of MULTIPLEXED Illumina data, a la Jimmy
# Modified for the USF 16S sequences.  Modifications were made by Reiko Michisaki MBARI, Dec 2015, Jan 2016

echo
echo
echo -e '\t' "\x20\xf0\x9f\x8f\x84" " "  "\xc2\xa1" BANZAI !
echo
echo


################################################################################
# CHECK FOR RAW DATA
################################################################################

# Define a variable called START_TIME
START_TIME=$(date +%Y%m%d_%H%M)

# Find the directory this script lives in, so it can find its friends.
#SCRIPT_DIR="$(dirname "$0")"
SCRIPT_DIR="/home/mbonteam/dev"
MY_SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
DATA_DIR="$MY_SCRIPT_DIR/data"


# Read in the parameter file (was source "$SCRIPT_DIR/banzai_params.sh"; now argument 1)
param_file="${1}"
source "${param_file}"

# check if param file exists:
if [[ -s "${param_file}" ]] ; then
	echo "Reading analysis parameters from:"
	echo "${param_file}"
else
	echo
	echo 'ERROR! Could not find analysis parameter file. You specified the file path:'
	echo
	echo "${param_file}"
	echo
	echo 'That file is empty or does not exist. Aborting script.'
	exit
fi


# check if sequencing metadata exists
if [[ -s "${SEQUENCING_METADATA}" ]] ; then
	echo "Reading sequencing metadata from:"
	echo "${SEQUENCING_METADATA}"
else
	echo
	echo 'ERROR! Could not find sequencing metadata file. You specified the file path:'
	echo
	echo "${SEQUENCING_METADATA}"
	echo
	echo 'That file is empty or does not exist. Aborting script.'
	exit
fi
# make standard name prefix for OTU, XML, and biom files
STANDARD_PREFIX="${SANCTUARY}"_"${START_TIME}"_"${LOCUS}"
DB_NAME=$(basename $BLAST_DB)
#BIOM_FILE_NAME="${STANDARD_PREFIX}"_"${DB_NAME}"

# make an analysis directory with starting time timestamp
ANALYSIS_DIR="${ANALYSIS_DIRECTORY}"/Analysis_"${START_TIME}"
mkdir "${ANALYSIS_DIR}"

# Write a log file of output from this script (everything that prints to terminal)
LOGFILE="${ANALYSIS_DIR}"/logfile.txt
exec > >(tee "${LOGFILE}") 2>&1

# check for correct newline characters (CRLF will break things)
echo 'Checking metadata file format for' "${SEQUENCING_METADATA}"
source "${SCRIPT_DIR}"/scripts/newline_fix.sh "${SEQUENCING_METADATA}"
if [[ -s "${NEWLINES_FIXED}" ]]; then
	SEQUENCING_METADATA="${NEWLINES_FIXED}"
fi

################################################################################
# CHECK FOR DEPENDENCIES
################################################################################
dependencies=($( echo pear cutadapt vsearch swarm seqtk python blastn R ))
echo 'Checking for dependencies:' "${dependencies[@]}"
for i in "${dependencies[@]}"; do
	if hash "${i}" 2>/dev/null; then
	# if command -v "${i}" >/dev/null 2>&1; then
		echo 'Found program' "${i}" 'in' $( which "${i}" )
	else
		echo 'ERROR: A program on which this script depends was not found:' "${i}"
		echo 'Aborting script.'
		exit
	fi
done

# Specify compression utility
if hash pigz 2>/dev/null; then
	ZIPPER="pigz"
	echo "pigz installation found"
else
	ZIPPER="gzip"
	echo "pigz installation not found; using gzip"
fi

# determine cpu load
#jnkstr=$(uptime | cut -d':' -f5)
#load1min=$(echo $jnkstr | cut -d',' -f1) 
#load5min=$(echo $jnkstr | cut -d',' -f2)
#load15min=$(echo $jnkstr | cut -d',' -f3)
#load=$(echo "scale=9; $load1min + $load5min + $load15min" | bc)
#avg_load=$(echo "scale=9; $load/3" | bc)
#ncore=$(echo "scale=9; n_core-4-$avg_load" | bc)
#n_cores=${ncore%%.*}
#echo ${p%%.*}
# Detect number of cores on machine; set variable
n_cores=$(getconf _NPROCESSORS_ONLN)
if [ $n_cores -gt 1 ]; then
	echo "$n_cores cores detected; Running 8"
	n_cores=8
else
	n_cores=1
	echo "Multiple cores not detected."
fi


echo "Analysis started at ""${START_TIME}" " and is located in ""${ANALYSIS_DIR}"

echo "Running script from: "
echo "$BASH_SOURCE"
echo "Parameters read from: "
echo "${param_file}"
echo "metadata read from: "
echo "${SEQUENCING_METADATA}"

# Copy these files into that directory as a verifiable log you can refer back to.
cp $BASH_SOURCE "${ANALYSIS_DIR}"/"${STANDARD_PREFIX}"_analysis_script.txt
cp "${param_file}" "${ANALYSIS_DIR}"/"${STANDARD_PREFIX}"_analysis_parameters.txt
cp "${SEQUENCING_METADATA}"  "${ANALYSIS_DIR}"/"${STANDARD_PREFIX}"_analysis_metadata.csv
################################################################################
# CHECK METADATA FILE FORMAT AND FOR DUPLICATE SAMPLE  NAMES
################################################################################

echo 'Checking for duplicate sample names.'
SID_COL=$(awk -F',' -v SID_COL_NAME=$ColumnName_SampleName '{for (i=1;i<=NF;i++) if($i == SID_COL_NAME) print i; exit}' $SEQUENCING_METADATA)
SID=$(awk -F',' -v SIDCOL=$SID_COL 'NR>1 {print $SIDCOL}' $SEQUENCING_METADATA | uniq)
duplicates=($(printf "${SID}" | sort | uniq -d))
#echo "$(printf "%s\n" "${#SID[*]}" )"
#echo "$(printf "%s\n" "${SID[@]}" )"
#echo "$(printf "%s\n" "${#duplicates[*]}" )"

#if (( uniqueNum != ${#SID[@]} )); then
if (( ${#duplicates[@]} )); then 
	echo " "
    echo 'FATAL ERROR:' "${ColumnName_SampleName}" 'duplicates found.' "${ColumnName_SampleName}" 'must be unique or the biom conversion will fail.'

#	echo "dup#" ${#duplicates[@]} " dup " $duplicates
	echo 'Duplicate(s):' "${duplicates[@]}"
	
	echo 'FATAL ERROR: Exiting program'
	echo " "
	exit 1
else
	echo " "
fi

################################################################################
# LOAD MULTIPLEX TAGS
################################################################################
TAG_COL=$(awk -F',' -v TAG_COL_NAME=$TAG_COLUMN_NAME '{for (i=1;i<=NF;i++) if($i == TAG_COL_NAME) print i; exit}' $SEQUENCING_METADATA)
TAGS=$(awk -F',' -v TAGCOL=$TAG_COL 'NR>1 {print $TAGCOL}' $SEQUENCING_METADATA | uniq)
ii=0
for T in $TAGS; do
	TAGS_SEQ[ii]=$T
	ii=$ii+1
done
tag_length=${#TAGS_SEQ[1]}
#echo "${TAGS}"
#R1_COL=$(awk -F',' -v R1_COL_NAME=$R1_COLUMN_NAME '{for (i=1;i<=NF;i++) if($i == R1_COL_NAME) print i; exit}' $SEQUENCING_METADATA)
#R1=$(awk -F',' -v R1COL=$R1_COL 'NR>1 {print $R1COL}' $SEQUENCING_METADATA | uniq)
#R2_COL=$(awk -F',' -v R2_COL_NAME=$R2_COLUMN_NAME '{for (i=1;i<=NF;i++) if($i == R2_COL_NAME) print i; exit}' $SEQUENCING_METADATA)
#R2=$(awk -F',' -v R2COL=$R2_COL 'NR>1 {print $R2COL}' $SEQUENCING_METADATA | uniq)
#echo "${R1}"
#TAGS=$(awk -F',' -v TAGCOL=$TAG_COL 'NR>1 {print $TAGCOL}' $SEQUENCING_METADATA | sort | uniq)
N_index_sequences=$(echo $TAGS | awk '{print NF}')

# check if number of tags is greater than one:
# if [[ "${N_index_sequences}" -gt 1 ]]; then
	 echo "Multiplex tags read from sequencing metadata (""${N_index_sequences}"") total"
# else
  # echo
  # echo 'ERROR:' "${N_index_sequences}" 'index sequences found. There should probably be more than 1.'
  # echo
  # echo 'Aborting script.'
	# exit
# fi

declare -a TAGS_ARRAY=($TAGS)


################################################################################
# Read in primers and create reverse complements. R2_COLUMN_NAME="R2"
################################################################################
PRIMER1_COLNUM=$(awk -F',' -v PRIMER1_COL=$PRIMER_1_COLUMN_NAME '{for (i=1;i<=NF;i++) if($i == PRIMER1_COL) print i; exit}' $SEQUENCING_METADATA)
PRIMER2_COLNUM=$(awk -F',' -v PRIMER2_COL=$PRIMER_2_COLUMN_NAME '{for (i=1;i<=NF;i++) if($i == PRIMER2_COL) print i; exit}' $SEQUENCING_METADATA)
PRIMER1=$(awk -F',' -v PRIMER1_COL=$PRIMER1_COLNUM 'NR==2 {print $PRIMER1_COL}' $SEQUENCING_METADATA)
PRIMER2=$(awk -F',' -v PRIMER2_COL=$PRIMER2_COLNUM 'NR==2 {print $PRIMER2_COL}' $SEQUENCING_METADATA)
#let's try to get all primers in case the dataset is not from the same sequencing run, re: mix and match
PRIME1=($(awk -F',' -v PRIMER1_COL=$PRIMER1_COLNUM 'NR>1 {print $PRIMER1_COL}' $SEQUENCING_METADATA))
PRIME2=($(awk -F',' -v PRIMER2_COL=$PRIMER2_COLNUM 'NR>1 {print $PRIMER2_COL}' $SEQUENCING_METADATA))

echo "Primers read from sequencing metadata:" "${PRIME1}" "${PRIME2}"

if [[ -n "${PRIMER1}" && -n "${PRIMER2}" ]]; then
  echo 'Primers read from metadata columns' "${PRIMER1_COLNUM}" 'and' "${PRIMER2_COLNUM}"
  echo 'Primer sequences:' "${PRIMER1}" "${PRIMER2}"
else
  echo 'ERROR:' 'At least one primer is not valid'
  echo 'Looked in metadata columns' "${PRIMER1_COLNUM}" 'and' "${PRIMER2_COLNUM}"
  echo 'Aborting script'
  exit
fi

# make primer array
read -a primers_arr <<< $( echo $PRIMER1 $PRIMER2 )

# Reverse complement primers
PRIMER1RC=$( echo ${PRIMER1} | tr "[ABCDGHMNRSTUVWXYabcdghmnrstuvwxy]" "[TVGHCDKNYSAABWXRtvghcdknysaabwxr]" | rev )
PRIMER2RC=$( echo ${PRIMER2} | tr "[ABCDGHMNRSTUVWXYabcdghmnrstuvwxy]" "[TVGHCDKNYSAABWXRtvghcdknysaabwxr]" | rev )
PRIME1RC=($( echo ${PRIME1} | tr "[ABCDGHMNRSTUVWXYabcdghmnrstuvwxy]" "[TVGHCDKNYSAABWXRtvghcdknysaabwxr]" | rev ))
PRIME2RC=($( echo ${PRIME2} | tr "[ABCDGHMNRSTUVWXYabcdghmnrstuvwxy]" "[TVGHCDKNYSAABWXRtvghcdknysaabwxr]" | rev ))


# make primer array
read -a primersRC_arr <<< $( echo $PRIMER1RC $PRIMER2RC )
echo 'PRIMER2RC=' "${PRIMER2RC}"
################################################################################
# Calculate the expected size of the region of interest, given the total size of fragments, and the length of primers and tags
################################################################################
EXTRA_SEQ=${TAGS_ARRAY[0]}${TAGS_ARRAY[0]}$PRIMER1$PRIMER2
LENGTH_ROI=$(( $LENGTH_FRAG - ${#EXTRA_SEQ} ))
LENGTH_ROI_HALF=$(( $LENGTH_ROI / 2 ))
echo "**** Extra_seq=", "${EXTRA_SEQ}", " Length_ROI=", "${LENGTH_ROI}"

################################################################################
# Find raw sequence files
################################################################################
# Look for any file with '.fastq' in the name in the parent directory
# note that this will include ANY file with fastq -- including QC reports!
# TODO make LIBRARY_DIRECTORIES an array by wrapping it in ()
LIBRARY_DIRECTORIES=$( find "$PARENT_DIR" -name '*.fastq*' -print0 | xargs -0 -n1 dirname | sort --unique )

# Count library directories and print the number found
# TODO if LIBRARY_DIRECTORIES is an array, its length is "${#LIBRARY_DIRECTORIES[@]}"
N_library_dir=$(echo $LIBRARY_DIRECTORIES | awk '{print NF}')
echo "${N_library_dir}"" library directories found:"

# Show the libraries that were found:
# TODO for i in "${LIBRARY_DIRECTORIES[@]}"; do echo "${i##*/}" ; done
#for i in $LIBRARY_DIRECTORIES; do echo "${i##*/}" ; done

# Assign it to a variable for comparison
LIBS_FROM_DIRECTORIES=$(for i in $LIBRARY_DIRECTORIES; do echo "${i##*/}" ; done)

# Read library names from file or sequencing metadata
if [ "${READ_LIB_FROM_SEQUENCING_METADATA}" = "YES" ]; then
	LIB_COL=$(awk -F',' -v LIB_COL_NAME=$LIBRARY_COLUMN_NAME '{for (i=1;i<=NF;i++) if($i == LIB_COL_NAME) print i; exit}' $SEQUENCING_METADATA)
	LIBS=$(awk -F',' -v LIBCOL=$LIB_COL 'NR>1 {print $LIBCOL}' $SEQUENCING_METADATA | uniq)
	N_libs=$(echo $LIBS | awk '{print NF}')
	echo "Library names read from sequencing metadata (""${N_libs}"") total"
	echo "${LIBS}"
else
	LIBS=$(tr '\n' ' ' < "${LIB_FILE}" )
	N_libs=$(echo $LIBS | awk '{print NF}')
	echo "Library names read from lib file (""${LIBS}"") total"
fi
# make library names into an array
# TODO LIBS_ARRAY is never used
# declare -a LIBS_ARRAY=($LIBS)

# Check that library names are the same in the metadata and file system
if [ "$LIBS_FROM_DIRECTORIES" != "$LIBS" ]; then
	echo "Warning: Library directories and library names in metadata are NOT the same. Something will probably go wrong later..."
else
	echo "Library directories and library names in metadata are the same - great jorb."
fi


# Unique samples are given by combining the library and tags
# TODO originally contained sort | uniq; this is unnecessary I think
#LIB_TAG_MOD=$( awk -F',' -v LIBCOL=$LIB_COL -v TAGCOL=$TAG_COL 'NR>1 { print "lib_" $LIBCOL "_tag_" $TAGCOL }' $SEQUENCING_METADATA | sort | uniq )
LIB_TAG_MOD_COL=$(awk -F',' -v LIB_TAG_MOD_COL_NAME=$LIBRARY_TAG_COMBO_COLUMN_NAME '{for (i=1;i<=NF;i++) if($i == LIB_TAG_MOD_COL_NAME) print i; exit}' $SEQUENCING_METADATA)
LIB_TAG_MOD=$(awk -F',' -v LIBTAGMODCOL=$LIB_TAG_MOD_COL 'NR>1 {print $LIBTAGMODCOL}' $SEQUENCING_METADATA)
#echo "LIB_TAG_MOD" "$LIB_TAG_MOD"
# create a file to store tag efficiency data
TAG_COUNT="${ANALYSIS_DIR}"/tag_count.txt
echo "library   tag   left_tagged   right_tagged" >> "${TAG_COUNT}"

# Clear out data directory
rm "${DATA_DIR}"/*.fastq*

################################################################################
# BEGIN LOOP TO PERFORM LIBRARY-LEVEL ACTIONS
# BOG data are on a network disk.  Copy file to mbonserver, process, then remove
################################################################################
ii=0
for C_LIB in $LIBS; do
	echo "Library " "${PARENT_DIR}"/"${C_LIB}"
	raw_files=($( find "${PARENT_DIR}"/"${C_LIB}" -name '*.fastq*' ))
 	for myfile in "${raw_files[@]}"; do
	 echo $(date +%H:%M) 'copying ' "${myfile}" "${DATA_DIR}"/"${myfile##*/}" 
	 cp "${myfile}"  "${DATA_DIR}"/"${myfile##*/}"
	 if [[ "${myfile}" =~ \.gz$ ]]; then
		 echo $(date +%H:%M) 'decompressing ' "${ZIPPER}" -d "${DATA_DIR}"/"${myfile##*/}"
		 "${ZIPPER}" -d "${DATA_DIR}"/"${myfile##*/}"
	 fi
	done	
   
   CURRENT_LIB="${DATA_DIR}"
   
  echo "find READS in " "${CURRENT_LIB}"
 # Identify the forward and reverse fastq files.
	READS=($(find "${CURRENT_LIB}" -name '*.fastq*' | sort))
	READ1="${READS[0]}"
	READ2="${READS[1]}"
	LIB_OUTPUT_DIR="${ANALYSIS_DIR}"/${C_LIB}
	echo "${LIB_OUTPUT_DIR}"
	mkdir -p "${LIB_OUTPUT_DIR}"


	##############################################################################
	# MERGE PAIRED-END READS AND QUALITY FILTER (PEAR)
	##############################################################################

	##############################################################################
	# CALCULATE EXPECTED AND MINIMUM OVERLAP OF PAIRED END SEQUENCES
	##############################################################################
	LENGTH_READ=$( head -n 100000 "${READ1}" | awk '{print length($0);}' | sort -nr | uniq | head -n 1 )
	#echo " **** DEBUG " "${LENGTH_READ}"
	OVERLAP_EXPECTED=$(($LENGTH_FRAG - (2 * ($LENGTH_FRAG - $LENGTH_READ) ) ))
	

	if [ "$ALREADY_PEARED" = "YES" ]; then
		MERGED_READS="$PEAR_OUTPUT"
		echo "Paired reads have already been merged."
	else
		echo $(date +%H:%M) "Merging reads in library" "${C_LIB}""..."
		MERGED_READS_PREFIX="${LIB_OUTPUT_DIR}"/1_merged
		MERGED_READS="${LIB_OUTPUT_DIR}"/1_merged.assembled.fastq
		pear \
			--forward-fastq "${READ1}" \
			--reverse-fastq "${READ2}" \
			--output "${MERGED_READS_PREFIX}" \
			-v $MINOVERLAP \
			-m $ASSMAX \
			-n $ASSMIN \
			-t $min_seq_length \
			-q $Quality_Threshold \
			-u $UNCALLEDMAX \
			-g $TEST \
			-p $PVALUE \
			-s $SCORING \
			-j $n_cores

		# check pear output:
		if [[ ! -s "${MERGED_READS}" ]] ; then
		    echo 'ERROR: No reads were merged.'
		    echo 'Maybe the script should exit, but this could be a library-specific problem.'
		fi

	find "${CURRENT_LIB}" -type f -name '*.fastq' -exec ${ZIPPER} "{}" \;

	fi

	################################################################################
	# EXPECTED ERROR FILTERING (usearch)
	################################################################################
	# FILTER READS (This is the last step that uses quality scores, so convert to fasta)
	if [ "${Perform_Expected_Error_Filter}" = "YES" ]; then
		echo $(date +%H:%M) "Filtering merged reads..."
		FILTERED_OUTPUT="${LIB_OUTPUT_DIR}"/2_filtered.fasta
		vsearch \
			--fastq_filter "${MERGED_READS}" \
			--fastq_maxee "${Max_Expected_Errors}" \
			--fastaout "${FILTERED_OUTPUT}" \
			--fasta_width 0
	else
		# Convert merged reads fastq to fasta
		echo  $(date +%H:%M) "converting fastq to fasta..."
		FILTERED_OUTPUT="${MERGED_READS%.*}".fasta
		seqtk seq -A "${MERGED_READS}" > "${FILTERED_OUTPUT}"
	fi

	# Compress merged reads
  echo $(date +%H:%M) "Compressing PEAR output..."
  #find "${LIB_OUTPUT_DIR}" -type f -name '*.fastq' -exec ${ZIPPER} "{}" \;
  rm "${MERGED_READS}"
  echo $(date +%H:%M) "PEAR output compressed."





	if [ "${RENAME_READS}" = "YES" ]; then
		echo $(date +%H:%M) "Renaming reads in library" "${C_LIB}""..."
		# TODO remove whitespace from sequence labels?
		# sed 's/ /_/'

		# original (usearch7):	sed -E "s/ (1|2):N:0:[0-9]/_"${CURRENT_LIB##*/}"_/" "${FILTERED_OUTPUT}" > "${CURRENT_LIB}"/tmp.fasta
		# update for usearch8, which without warning removes any part of the sequence ID following a space.
		# holy shit this ads the _"${CURRENT_LIB##*/}"_ to EVERY line
		# sed -E "s/$/_"${CURRENT_LIB##*/}"_/" "${FILTERED_OUTPUT}" > "${CURRENT_LIB}"/tmp.fasta
		# sed -E "s/>([a-zA-Z0-9-]*:){4}/>/" "${CURRENT_LIB}"/tmp.fasta > "${FILTERED_OUTPUT%.*}"_renamed.fasta
		# rm "${CURRENT_LIB}"/tmp.fasta

		# updated 20150521; one step solution using awk; removes anything after the first space!
		FILTERED_RENAMED="${FILTERED_OUTPUT%.*}"_renamed.fasta
		awk -F'[: ]' '{
				if ( /^>/ )
					print ">"$4":"$5":"$6":"$7"'${C_LIB}'_tag_";
				else
					print $0
				}' "${FILTERED_OUTPUT}" > "${FILTERED_RENAMED}"

		FILTERED_OUTPUT="${FILTERED_RENAMED}"

		# rm "${FILTERED_OUTPUT}"

	else
		echo "Reads not renamed"
	fi

    echo FILTERED_OUTPUT="${FILTERED_RENAMED}"

	################################################################################
	# HOMOPOLYMERS (grep, awk)
	################################################################################
	if [ "${REMOVE_HOMOPOLYMERS}" = "YES" ]; then
		echo $(date +%H:%M) "Removing homopolymers..."
		HomoLineNo="${CURRENT_LIB}"/homopolymer_line_numbers.txt
		grep -E -i -B 1 -n "(A|T|C|G)\1{$HOMOPOLYMER_MAX,}" "${FILTERED_OUTPUT}" | \
			cut -f1 -d: | \
			cut -f1 -d- | \
			sed '/^$/d' > "${HomoLineNo}"
		if [ -s "${HomoLineNo}" ]; then
			DEMULTIPLEX_INPUT="${CURRENT_LIB}"/3_no_homopolymers.fasta
			awk 'NR==FNR{l[$0];next;} !(FNR in l)' "${HomoLineNo}" "${FILTERED_OUTPUT}" > "${DEMULTIPLEX_INPUT}"
			awk 'NR==FNR{l[$0];next;} (FNR in l)' "${HomoLineNo}" "${FILTERED_OUTPUT}" > "${CURRENT_LIB}"/homopolymeric_reads.fasta
		else
			echo "No homopolymers found" > "${CURRENT_LIB}"/3_no_homopolymers.fasta
			DEMULTIPLEX_INPUT="${FILTERED_OUTPUT}"
		fi
	else
		echo "Homopolymers not removed."
		DEMULTIPLEX_INPUT="${FILTERED_OUTPUT}"
	fi

	################################################################################
	# DEMULTIPLEXING (awk)
	# BOG 18S, COI, Raw sequences are already demultiplexed
	# Stanford 12S Needs to be demultiplexed
	#
	# Read loci from metadata file?
	################################################################################
	# make a directory to put all the demultiplexed files in
	DEMULTIPLEXED_DIR="${LIB_OUTPUT_DIR}"/demultiplexed
	echo mkdir "${DEMULTIPLEXED_DIR}"
	mkdir "${DEMULTIPLEXED_DIR}"

	# Copy sequences to fasta files into separate directories based on tag sequence on left side of read
	# TODO test for speed against removing the tag while finding it: wrap first tag regex in gsub(/pattern/,""):  awk 'gsub(/^.{0,9}'"$TAG_SEQ"'/,""){if . . .
	# 20150522 changed {0,9} to {3} to eliminate flexibility (that could result in a read being assigned to >1 sample)
	# awk '/^.{0,9}'"$TAG_SEQ"'/{if (a && a !~ /^.{0,9}'"$TAG_SEQ"'/) print a; print} {a=$0}' "${DEMULTIPLEX_INPUT}" > "${TAG_DIR}"/1_tagL_present.fasta ) &
	#if [ "${SECONDARY_INDEX}" = "YES" ]; then
	#echo CTAGS $CTAGS
	
	if [ "${INDEXED}" = "NO" ]; then
			echo $(date +%H:%M) "NON INDEXED Demultiplexing: removing tags and adding to sequence ID in library" "${C_LIB}""..."

	
		for TAG_SEQ in $TAGS; do
			(	TAG_DIR="${LIB_OUTPUT_DIR}"/demultiplexed/tag_"${TAG_SEQ}"
			echo "mkdir "${TAG_DIR}" line 484"
			mkdir "${TAG_DIR}"
			demult_file_L="${TAG_DIR}"/1_tagL_removed.fasta
			demult_file_R="${TAG_DIR}"/2_notags.fasta

		# Left side tag
			echo "Left side tag:" "${TAG_SEQ}" 
			awk 'gsub(/^.{3}'"$TAG_SEQ"'/,"") {
			if (a && a !~ /^.{3}'"$TAG_SEQ"'/)
				print a;
			print
		} {a=$0}' "${DEMULTIPLEX_INPUT}" > "${demult_file_L}"

		# Right side tag
		#				print a "tag_""'"$TAG_SEQ"'";

			TAG_RC=$( echo ${TAG_SEQ} | tr "[ATGCatgcNn]" "[TACGtacgNn]" | rev )
			echo "TAG_RC=" "${TAG_RC}"
			awk 'gsub(/'"$TAG_RC"'.{3}$/,"") {
			if (a && a !~ /'"$TAG_RC"'.{3}$/)
				print a "'"$TAG_SEQ"'";
			print
		} {a = $0}' "${TAG_DIR}"/1_tagL_removed.fasta > "${demult_file_R}"

			echo "${CURRENT_LIB##*/}" "${TAG_SEQ}" $(wc -l "${demult_file_L}" | \
			awk '{ print ($1/2) }') $(wc -l "${demult_file_R}" | \
			awk '{ print ($1/2)}') >> "${TAG_COUNT}" ) &

		done
		wait
			
		
	else #INDEXED == YES
	# process the BOG style sequences
		
		echo $(date +%H:%M) "Demultiplexing: removing tags and adding to sequence ID in library" "${C_LIB}""..."
		TAG_SEQ="${TAGS_SEQ[$ii]}"
			TAG_DIR="${LIB_OUTPUT_DIR}"/demultiplexed/tag_"${TAG_SEQ}"
			mkdir "${TAG_DIR}"
			demult_file_L="${TAG_DIR}"/1_tagL_removed.fasta
			demult_file_R="${TAG_DIR}"/2_notags.fasta
			echo 'demult_file_R=' "${demult_file_R}"
		# Raw sequences are already demultiplexed so skip this step
		# NOTE BOG samples/individual barcoded raw seq files MUST be run individually !!!!  re: TAGS
		
	#		echo " filter output sed tag " "$LIB_TAG_MOD"
	#		cat "${FILTERED_OUTPUT}" | sed 's/"${C_LIB}"_tag_/"${LIB_TAG_MOD}"/'> "${demult_file_R}"
			cat "${FILTERED_OUTPUT}" | sed 's/_tag_/_tag_'"${TAG_SEQ}"'/'> "${demult_file_R}"		
		



		wait
	fi #INDEXED


	################################################################################
	# PRIMER REMOVAL
	################################################################################
	# CUTADAPT modification: MBARI samples may have different primers in a pipeline run, mix and match samples
	# as such we put these in the library level processing loop. 
	# Remove PRIMER1 and PRIMER2 from the BEGINNING of the reads. NOTE cutadapt1.7+ will accept ambiguities in primers.

	
	#if [[ "${REMOVE_PRIMER}" == "YES" ]]; then
	if [ "${REMOVE_PRIMER}" = "YES" ]; then
		echo '[ "${REMOVE_PRIMER}" == "YES" ]'
		echo '**demult_file_R=' "${demult_file_R}"

			# count lines in primer removal input. FASTA_FILE=2_notags.fasta
		FASTA_FILE="${TAG_DIR}"/2_notags.fasta
		echo $(date +%H:%M) "Counting sequences in primer removal input..." "${FASTA_FILE}"
		seq_N_demult_concat=$( grep -e '^>' --count "${FASTA_FILE}" )
		echo $(date +%H:%M) "${seq_N_demult_concat}" "sequences found in file" "${FASTA_FILE}"

		# since these are indexed we need to loop through tags?
					# remove primer 1 from left side of sequences
			primerL1_removed="${TAG_DIR}"/5_primerL1_removed.fasta
			prime_1="${PRIME1[$ii]}"
			echo prime_1: "${prime_l}"
			# echo ' primerL1_removed="${primerL1_removed}"'
			# ( cutadapt \
				# -g ^"${PRIMER1}" \
				# -e "${PRIMER_MISMATCH_PROPORTION}" \
				# -m "${LENGTH_ROI_HALF}" \
				# --discard-untrimmed \
				# "${FASTA_FILE}" > "${primerL1_removed}" ) &
							echo ' primerL1_removed="${primerL1_removed}"'
			( cutadapt \
				-g ^"${PRIMER1}" \
				-e "${PRIMER_MISMATCH_PROPORTION}" \
				-m "${LENGTH_ROI_HALF}" \
				--discard-untrimmed \
				"${FASTA_FILE}" > "${primerL1_removed}" ) &


			wait


			# check for cutadapt/primer removal success.
			if [[ ! -s "${primerL1_removed}" ]]; then
			  echo 'ERROR: cutadapt did not process reads correctly. This file is empty or absent:'
				echo "${primerL1_removed}"
			  echo 'Aborting script'
			  exit
			fi


			# Remove the reverse complement of primer 1 from the right side of sequences
			primerR1_removed="${TAG_DIR}"/6_primerR1_removed.fasta
			prime_2="${PRIME2RC[$ii]}"

			
			( cutadapt \
				-a "${PRIMER2RC}"$ \
				-e "${PRIMER_MISMATCH_PROPORTION}" \
				-m "${LENGTH_ROI_HALF}" \
				--discard-untrimmed \
				"${primerL1_removed}" > "${primerR1_removed}" ) &

			wait

			# check for cutadapt/primer removal success.
			if [[ ! -s "${primerR1_removed}" ]]; then
				echo 'ERROR: cutadapt did not process reads correctly. This file is empty or absent:'
				echo "${primerR1_removed}"
				echo 'Aborting script'
				exit
			fi

			# Remove reverse-complement the sequences in which the RC of primer 1 was found on the right side
			seqtk seq -r "${TAG_DIR}"/6_primerR1_removed.fasta > "${TAG_DIR}"/6_primerR1_removedRC.fasta

			# paste together the contents of the files that primers were removed from.
			DEREP_INPUT="${TAG_DIR}"/7_no_primers.fasta

			#cat "${TAG_DIR}"/6_primerR1_removedRC.fasta "${TAG_DIR}"/6_primerR2_removed.fasta > "${DEREP_INPUT}"
			cat "${TAG_DIR}"/6_primerR1_removedRC.fasta > "${DEREP_INPUT}"
			
			
		
	fi  #	if [[ "${REMOVE_PRIMER}" == "YES" ]]; then



	#remove the unzipped fastq sequence data files
    rm "${DATA_DIR}"/*.fastq*
	ii=$ii+1    

done
################################################################################
# END LOOP TO PERFORM LIBRARY-LEVEL ACTIONS
################################################################################





# TODO add single if/else for CONCATENATE_SAMPLES: assign directory as appropriate, correct references within loop to be extendable
if [ "$CONCATENATE_SAMPLES" = "YES" ]; then
	# do the stuff here
	WORKING_DIR="${CONCAT_DIR}"
else
	WORKING_DIR="${LIBRARY_DIRECTORIES}"
fi
	if [[ "${REMOVE_PRIMER}" == "YES" ]]; then
		CAT_FASTA=7_no_primers.fasta
		else
		CAT_FASTA=2_notags.fasta
	fi
echo  'CAT_FASTA=' "${CAT_FASTA}" 
		

################################################################################
# CONCATENATE SAMPLES
################################################################################
# TODO could move this first step up above any loops (no else)
if [ "$CONCATENATE_SAMPLES" = "YES" ]; then
# assuming that we are working with indexed samples which is why they need to be concatenated?
	# TODO MOVE THE VARIABLE ASSIGNMENT TO TOP; MOVE MKDIR TO TOP OF CONCAT IF LOOP
	CONCAT_DIR="$ANALYSIS_DIR"/all_lib
	mkdir "${CONCAT_DIR}"
	CONCAT_FILE="${CONCAT_DIR}"/1_demult_concat.fasta
	echo $(date +%H:%M) "Concatenating fasta files..." "${CONCAT_FILE}"

	# TODO could move this into above loop after demultiplexing?
	# do we have 1 library directory (Stanford) or many (BOG/USF)
	if [[ N_libs -gt 1 ]]; then
		ii=0
		for CURRENT_LIB in $LIBS; do

			LIB_OUTPUT_DIR="${ANALYSIS_DIR}"/${CURRENT_LIB}

			TAG_SEQ="${TAGS_SEQ[$ii]}"
			cat "${LIB_OUTPUT_DIR}"/demultiplexed/tag_"${TAG_SEQ}"/"${CAT_FASTA}" >> "${CONCAT_FILE}"
			echo "${LIB_OUTPUT_DIR}"/demultiplexed/tag_"${TAG_SEQ}"/"${CAT_FASTA}" 
			ii=$ii+1
		done

	else
		for CURRENT_LIB in $LIBS; do

			LIB_OUTPUT_DIR="${ANALYSIS_DIR}"/${CURRENT_LIB##*/}

			for TAG_SEQ in $TAGS; do
				cat "${LIB_OUTPUT_DIR}"/demultiplexed/tag_"${TAG_SEQ}"/2_notags.fasta >> "${CONCAT_FILE}"
			done

			echo $(date +%H:%M) "Compressing fasta files..."
			find "${LIB_OUTPUT_DIR}" -type f -name '*.fasta' -exec ${ZIPPER} "{}" \;
			echo $(date +%H:%M) "fasta files compressed."

		done		
	fi

	

	################################################################################
	# Count the occurrences of '_tag_' + the 12 character bar code tag following it in the concatenated file
	################################################################################
  # TODO !!! This will fail if there are underscores in the library names !!!
	# an attempt at making this robust to underscores
	# grep -E -o '_lib_.+?(?=_tag)_tag_.{6}' "${CONCAT_DIR}"/1_demult_concat.fasta | sed 's/_lib_//;s/_tag_/ /' | sort | uniq -c | sort -nr > "${CONCAT_DIR}"/1_demult_concat.fasta.tags
# DEBUG HERE, various tag lengths
	echo $(date +%H:%M) "Counting reads associated with each sample index (primer tag)..."
	tag_string='_tag_.{'"${tag_length}"'}'
		grep -E -o "${tag_string}" "${CONCAT_DIR}"/1_demult_concat.fasta | sort | uniq -c | sort -nr > "${CONCAT_DIR}"/1_demult_concat.fasta.tags

		#	grep -E -o '_tag_.{12}' "${CONCAT_DIR}"/1_demult_concat.fasta | sed 's/_lib_//;s/_tag_/ /' | sort | uniq -c | sort -nr > "${CONCAT_DIR}"/1_demult_concat.fasta.tags

		#grep -E -o '_lib_[^_]*_tag_.{6}' "${CONCAT_DIR}"/1_demult_concat.fasta | sed 's/_lib_//;s/_tag_/ /' | sort | uniq -c | sort -nr > "${CONCAT_DIR}"/1_demult_concat.fasta.tags
	# CHANGING TAG LIBRARY LABEL HERE
	sed -i 's/_tag_/_/' "${CONCAT_DIR}"/1_demult_concat.fasta
	echo $(date +%H:%M) "Summary of sequences belonging to each sample index found in ""${CONCAT_DIR}""/1_demult_concat.fasta.tags"
	################################################################################


# This is where CUTADAPT was
# if REMOVE_PRIMER

	# else # no REMOVE_PRIMER ::::::::::::::::::::::::::::::::::::::::::::::
	DEREP_INPUT="${CONCAT_DIR}"/1_demult_concat.fasta
# fi

		
	# check that it worked (derep input / no primers)
	if [[ ! -s "${DEREP_INPUT}" ]] ; then
	    echo 'ERROR: Input file for dereplication is empty or absent.'
	    echo 'This will cause problems for all remaining steps, so script will exit.'
	    exit
	fi


	################################################################################
	# CONSOLIDATE IDENTICAL SEQUENCES (DEREPLICATION)
	################################################################################
	echo $(date +%H:%M) "Identifying identical sequences... python /home/mbonteam/dev/dereplication/dereplicate_fasta.py" "${DEREP_INPUT}"
# cutadapt section puts blank lines in 7_no_primers.fast which makes the dereplication fail
	sed -i '/^$/d' "${CONCAT_FILE}"

	derep_output="${DEREP_INPUT}".derep
	python "/home/mbonteam/dev/dereplication/dereplicate_fasta.py" "${DEREP_INPUT}"

	# check for derep output
	if [[ ! -s "${derep_output}" ]] ; then
	    echo 'ERROR: python dereplication output is empty or absent.'
	    echo 'This will cause problems for all remaining steps, so script will exit.'
	    exit
	fi


	# Exclude singleton sequences (if NF > 2), count the number of sequences per duplicate (print NF-1), sort them by the number of sequences per duplicate (sort -nr), and precede with a name ("DUP_X", where X is the line number,sorted)
	echo $(date +%H:%M) "Counting duplicates per identical sequence and excluding singletons... (awk)"
	no_singletons="${DEREP_INPUT%/*}"/nosingle.txt

	awk -F';' '{
		if (NF > 2)
			print NF-1 ";" $0
		}' "${derep_output}" | \
	sort -nr | \
	awk -F';' '{
		print ">DUP_" NR ";" $0
	}' > "${no_singletons}"

	# check output
	if [[ ! -s "${no_singletons}" ]] ; then
	    echo 'There was a problem generating the nosingletons file. It is empty or absent.'
	    echo 'This will cause problems counting sequences for dereplication.'
	fi



	# COUNT OCCURRENCES PER SAMPLE (LIBRARY + TAG) PER DUPLICATE

	# assign a path for the output (a table of counts of each duplicate sequence in each unique combination of library and primer ("tag") indexes, and a fasta of all the duplicate sequences.
	duplicate_table="${DEREP_INPUT%/*}"/duplicate_table.csv
	duplicate_fasta="${DEREP_INPUT%/*}"/duplicates.fasta

	# make a directory to store the temporary duplicate files
	temp_dir="${DEREP_INPUT%/*}"/dup_temp
	mkdir "${temp_dir}"

	# set a file prefix for the batches of samples
	sample_batch_prefix="${temp_dir}"/sample_batch_

	# split the sample identifiers (lib + tag combination) into batches of no more than the number of available cores
	echo $LIB_TAG_MOD | tr ' ' '\n' | split -l "${n_cores}" - "${sample_batch_prefix}"


	# for each of the batches of files
	for batch in "${sample_batch_prefix}"* ; do

		echo processing batch "${batch##*/}"

		# 	current_batch=$( cat "${batch}" ) # this reads whitespace rather than newline

		for sample in $( cat "$batch" ) ; do

			# say that it's being processed
			echo $(date +%H:%M) "Processing" "${sample}""..."

			# Isolate this process to be put in the background
			(

			# write an output file called *.dup, start by printing the lib/tag being processed, then print a count the occurrences of the current lib/tag on each line of the input file
			awk 'BEGIN {print "'$sample'" ; FS ="'${sample}'" } { print NF -1 }' "${no_singletons}" > "${temp_dir}"/"${sample}".dup

			) &

		done

		wait

	done

	# write a file of names of each of the duplicates:
	dupnames="${temp_dir}"/dupnames
	awk -F';' 'BEGIN {print "sample"} {print $1}' "$no_singletons" | sed 's/>//' > "${dupnames}"

	# first, count the number of duplicate files:
	n_files=$(find "${temp_dir}" -type f -name '*.dup*' | wc -l)

	# I think this was only relevant for a different approach
	max_files=$(ulimit -n)

	# this will paste row by row... takes 48s on a set of 300 files (samples) each containing 630023 lines (duplicates)
	paste -s -d, "${dupnames}" "${temp_dir}"/*.dup > "${duplicate_table}"

	# this will do columns; it takes a very long time.
	# for file in "${temp_dir}"/*; do cat final.dup | paste - $file >temp; cp temp final.dup; done; rm temp

	# cleanup
	# rm "${infile%/*}"/*.dup
	# rm "${sample_batch_prefix}"*

	# say that you're finished.
	echo $(date +%H:%M) "Identical sequences consolidated in file ""${duplicate_table}"

	
	# Write fasta file in order to blast sequences
	echo $(date +%H:%M) "Writing fasta file of duplicate sequences"
	awk -F';' '{ print $1 ";size=" $2 ";\n" $3 }' "${no_singletons}" > "${duplicate_fasta}"

	# check if duplicate fasta and duplicate table exist. (Might need to check size)
	if [[ ! -s "${duplicate_fasta}" ]] ; then
	    echo 'There was a problem generating the duplicate fasta file. It is empty or absent.'
	    echo 'The remainder of the script, including OTU clustering, depends on this file.'
	    echo 'Aborting script.'
	    exit
	fi
	if [[ ! -s "${duplicate_table}" ]] ; then
	    echo 'There was a problem generating the duplicate table. It is empty or absent.'
	    echo 'Aborting script.'
	    exit
	fi

################################################################################


	################################################################################
	# CLUSTER OTUS
	################################################################################
	# Note that identical (duplicate) sequences were consolidated earlier;
	# This step outputs a file (*.uc) that lists, for every sequence, which sequence it clusters with
	if [ "$CLUSTER_OTUS" = "NO" ]; then
		BLAST_INPUT="${duplicate_fasta}"
	else
		echo $(date +%H:%M) "Clustering OTUs..."

		case "${cluster_method}" in

		    "swarm" )

		        echo $(date +%H:%M) 'Clustering sequences into OTUs using swarm'
		        source "${SCRIPT_DIR}"/OTU_clustering/cluster_swarm.sh "${duplicate_fasta}"

		    ;;

		    "vsearch" )

		        # echo $(date +%H:%M) 'Clustering sequences into OTUs using vsearch'
		        # source "${SCRIPT_DIR}"/OTU_clustering/cluster_vsearch.sh "${duplicate_fasta}"
						echo "Sorry, OTU clustering with vsearch has not been implemented yet."
						echo $(date +%H:%M) 'Clustering sequences into OTUs using swarm'
		        source "${SCRIPT_DIR}"/OTU_clustering/cluster_swarm.sh "${duplicate_fasta}"

		    ;;

		    "usearch" )

		        echo $(date +%H:%M) 'Clustering sequences into OTUs using usearch'
		        source "${SCRIPT_DIR}"/OTU_clustering/cluster_usearch.sh "${duplicate_fasta}"

		    ;;

		    * )

		        echo "${cluster_method}" 'is an invalid clustering method.'
		        echo 'Must be one of swarm, vsearch, usearch, or none.'
		        echo $(date +%H:%M) 'Clustering sequences into OTUs using swarm'
		        source "${SCRIPT_DIR}"/OTU_clustering/cluster_swarm.sh "${duplicate_fasta}"

		    ;;

		esac

		# check that dup to otu map is greater than 12 bytes
		minsize=12
		size_dup_otu_map=$(wc -c <"${dup_otu_map}")
		if [ $size_dup_otu_map -lt $minsize ]; then
		    echo 'There was an error generating the dup-to-otu map.'
		fi


		# Assign the path for the OTU table
		# OTU_table="${dir_out}"/OTU_table.csv

		# Convert duplicate table to OTU table using R script (arguments: (1) duplicate table, (2) dup to otu table, (3) otu table path
		echo 'dup_to_OTU_table.R processing ...'
		Rscript "$SCRIPT_DIR/dup_to_OTU_table.R" "${duplicate_table}" "${dup_otu_map}" "${OTU_table}"

		# check if OTU table and OTU fasta exist (and/or are of size gt 1?)
		if [[ ! -s "${OTU_fasta}" ]] ; then
		    echo 'There was a problem generating the OTU fasta file. It is empty or absent.'
		    echo 'Aborting script.'
		    exit
		fi
		if [[ ! -s "${OTU_table}" ]] ; then
		    echo 'There was a problem generating the OTU table. It is empty or absent.'
		    echo 'Aborting script.'
		    exit
		fi

	fi

	##############################################################################
	# CHECK FOR CHIMERAS
	##############################################################################
	if [[ "${remove_chimeras}" = "YES" ]] ; then
		echo $(date +%H:%M) 'Looking for chimeras in OTU fasta file using vsearch'
		source "${SCRIPT_DIR}"/chimera_check.sh "${OTU_fasta}"
		BLAST_INPUT="${chimera_free_fasta}"
	else
		BLAST_INPUT="${OTU_fasta}"
	fi
otu_cnt=$(grep -o  'DUP_' "$ANALYSIS_DIR"/all_lib/OTUs_swarm/OTU_table.csv  | wc -l)
otu_fasta_cnt=$(grep -o  'DUP_' "${BLAST_INPUT}"  | wc -l)
	################################################################################
	# BLAST CLUSTERS
	################################################################################
	#blast_output="${DEREP_INPUT%/*}"/10_BLASTed.xml
	# removed following blastn parameter
	# 		-max_target_seqs "${MAXIMUM_MATCHES}" \

	blast_output="${DEREP_INPUT%/*}"/"${STANDARD_PREFIX}"_"${DB_NAME}".xml
	echo ":::::::::::::::::::::: BLAST ::::::::::::::::::::::::::::::::"
	echo $(date +%H:%M) " BLASTing..." 
	echo " Database: " "${BLAST_DB}"
	echo " Percent identity: " "${PERCENT_IDENTITY}"
	echo " Word size: " "${WORD_SIZE}"
	echo " E value: " "${EVALUE}"
#	echo " Maximum target matches: " "${MAXIMUM_MATCHES}"
	echo " Culling Limit: " "${culling_limit}"
	echo " Output format: 5"
	echo " Blast input: " "${BLAST_INPUT}"
	echo " Blast output: " "${blast_output}"
	
	blastn \
		-query "${BLAST_INPUT}" \
		-db "$BLAST_DB" \
		-num_threads "$n_cores" \
		-perc_identity "${PERCENT_IDENTITY}" \
		-word_size "${WORD_SIZE}" \
		-evalue "${EVALUE}" \
		-culling_limit "${culling_limit}" \
		-outfmt 5 \
		-out "${blast_output}"

	# check for blast output
	if [[ ! -s "${blast_output}"  ]]; then
		echo
		echo 'BLAST failed: the output file is empty or absent.'
	    echo 'File should be:' "${blast_output}"
		echo
	fi


else
################################################################################
# DON'T CONCATENATE SAMPLES
################################################################################

	################################################################################
	# PRIMER REMOVAL
	################################################################################
	echo $(date +%H:%M) "Removing primers..."
	for TAG_SEQ in $TAGS; do
		TAG_DIR="${ANALYSIS_DIR}"/demultiplexed/tag_"${TAG_SEQ}"
		# Remove PRIMER1 from the beginning of the reads. NOTE cutadapt1.7+ will accept ambiguities in primers.
		cutadapt -g ^"${PRIMER1}" -e "${PRIMER_MISMATCH_PROPORTION}" -m "${LENGTH_ROI_HALF}" --discard-untrimmed "${TAG_DIR}"/2_notags.fasta > "${TAG_DIR}"/5_primerL1_removed.fasta
		cutadapt -g ^"${PRIMER2}" -e "${PRIMER_MISMATCH_PROPORTION}" -m "${LENGTH_ROI_HALF}" --discard-untrimmed "${TAG_DIR}"/2_notags.fasta > "${TAG_DIR}"/5_primerL2_removed.fasta
		# Remove the primer on the other end of the reads by reverse-complementing the files and then trimming PRIMER1 and PRIMER2 from the left side.
		# NOTE cutadapt1.7 will account for anchoring these to the end of the read with $
		seqtk seq -r "${TAG_DIR}"/5_primerL1_removed.fasta | cutadapt -g ^"${PRIMER2}" -e "${PRIMER_MISMATCH_PROPORTION}" -m "${LENGTH_ROI_HALF}" --discard-untrimmed - > "${TAG_DIR}"/6_primerR1_removed.fasta
		seqtk seq -r "${TAG_DIR}"/5_primerL2_removed.fasta | cutadapt -g ^"${PRIMER1}" -e "${PRIMER_MISMATCH_PROPORTION}" -m "${LENGTH_ROI_HALF}" --discard-untrimmed - > "${TAG_DIR}"/6_primerR2_removed.fasta
		seqtk seq -r "${TAG_DIR}"/6_primerR1_removed.fasta > "${TAG_DIR}"/6_primerR1_removedRC.fasta
		cat "${TAG_DIR}"/6_primerR1_removedRC.fasta "${TAG_DIR}"/6_primerR2_removed.fasta > "${TAG_DIR}"/7_no_primers.fasta
	done

	################################################################################
	# CONSOLIDATE IDENTICAL SEQUENCES
	################################################################################
	for TAG_SEQ in $TAGS; do
		TAG_DIR="${ANALYSIS_DIR}"/demultiplexed/tag_"${TAG_SEQ}"

		DEREP_INPUT="${TAG_DIR}"/7_no_primers.fasta

		# usearch -derep_fulllength "${DEREP_INPUT}" -sizeout -strand both -uc "${TAG_DIR}"/derep.uc -output "${TAG_DIR}"/7_derep.fasta
		echo $(date +%H:%M) "Consolidating identical sequences..."
		python "$SCRIPT_DIR/dereplication/dereplicate_fasta.py" "${DEREP_INPUT}"

		# REMOVE SINGLETONS
		# usearch -sortbysize "${TAG_DIR}"/7_derep.fasta -minsize 2 -sizein -sizeout -output "${TAG_DIR}"/8_nosingle.fasta
		# COUNT DUPLICATES PER READ, REMOVE SINGLETONS
		awk -F';' '{ if (NF > 2) print NF-1 ";" $0 }' "${DEREP_INPUT}".derep | sort -nr | awk -F';' '{ print ">DUP_" NR ";" $0}' > ${DEREP_INPUT%/*}/nosingle.txt

		# count the duplicates
		awk 'BEGIN { FS ="_tag_'${TAG_SEQ}'" } { print NF -1 }' "${DEREP_INPUT%/*}"/nosingle.txt > ${DEREP_INPUT%/*}/"${TAG_SEQ}".dup
#		awk 'BEGIN { FS ="_tag_'${TAG_SEQ}'" } { print NF -1 }' "${DEREP_INPUT%/*}"/nosingle.txt > ${DEREP_INPUT%/*}/"${TAG_SEQ}".dup

		# Write fasta file in order to blast sequences
		awk -F';' '{ print $1 ";size=" $2 ";\n" $3 }' ${DEREP_INPUT%/*}/nosingle.txt > ${DEREP_INPUT%/*}/no_duplicates.fasta

		# CLUSTER SEQUENCES
		if [ "$CLUSTER_OTUS" = "NO" ]; then
			BLAST_INPUT=${DEREP_INPUT%/*}/no_duplicates.fasta
		else
			CLUSTER_RADIUS="$(( 100 - ${CLUSTERING_PERCENT} ))"
			UPARSE_OUT="${DEREP_INPUT%/*}"/OTU_uparse.txt
			usearch -cluster_otus "${DEREP_INPUT%/*}"/nosingle.txt -otu_radius_pct "${CLUSTER_RADIUS}" -sizein -sizeout -otus "${TAG_DIR}"/9_OTUs.fasta -uparseout "${UPARSE_OUT}"
			BLAST_INPUT="${TAG_DIR}"/9_OTUs.fasta
		fi

		# BLAST CLUSTERS
		blastn -query "${BLAST_INPUT}" -db "$BLAST_DB" -num_threads "$n_cores" -perc_identity "${PERCENT_IDENTITY}" -word_size "${WORD_SIZE}" -evalue "${EVALUE}" -max_target_seqs "${MAXIMUM_MATCHES}" -outfmt 5 -out "${TAG_DIR}"/"${STANDARD_PREFIX}"_"${DB_NAME}".xml
	done
fi


echo "Taxonomic annotion with MEGAN"
################################################################################
# TAXONOMIC ANNOTATION
################################################################################
if [ "$CONCATENATE_SAMPLES" = "YES" ]; then
	DIRECTORIES="${DEREP_INPUT%/*}"
else
	DIRECTORIES=$( find "${ANALYSIS_DIR}"/demultiplexed -type d -d 1 )
fi

for DIR in "$DIRECTORIES"; do

	# Some POTENTIAL OPTIONS FOR MEGAN EXPORT:
	# {readname_taxonname|readname_taxonid|readname_taxonpath|readname_matches|taxonname_count|taxonpath_count|taxonid_count|taxonname_readname|taxonpath_readname|taxonid_readname}
	# PERFORM COMMON ANCESTOR GROUPING IN MEGAN

		# check for blast output
	if [[ -s "${blast_output}"  ]]; then

		echo $(date +%H:%M) "${blast_output}" ',BLAST output found; proceeding to create Post_Blast file.'
		blast_fail="NO"

			POST_BLAST_FILE=Post_BLAST_processing_"$START_TIME".sh
			echo "POST_BLAST_FILE=" "${POST_BLAST_FILE}"
			echo SCRIPT_DIR="/home/mbonteam/dev" >> "$POST_BLAST_FILE"
			echo RSCRIPTS="/home/mbonteam/MBARI/reiko/scripts" >> "$POST_BLAST_FILE"
			#
			echo "###################################################################################################" >> "$POST_BLAST_FILE"
			#			echo # CHANGE THESE DIRECTORIES TO YOUR DIRECTORIES >> "$POST_BLAST_FILE"
			# "${STANDARD_PREFIX}"_"${DB_NAME}" BIOM_FILE_NAME="${STANDARD_PREFIX}"_"${DB_NAME}"
			echo  STANDARD_PREFIX="$STANDARD_PREFIX" >> "$POST_BLAST_FILE"
			echo  DB_NAME="$DB_NAME" >> "$POST_BLAST_FILE"
			echo  BIOM_FILE_NAME="$BIOM_FILE_NAME" >> "$POST_BLAST_FILE"
			echo  ANALYSIS_DIR="$ANALYSIS_DIR" >> "$POST_BLAST_FILE"
			echo  DIR="$ANALYSIS_DIR"/all_lib >> "$POST_BLAST_FILE"
			echo START_TIME="$START_TIME" >> "$POST_BLAST_FILE"
			echo param_file="$param_file" >> "$POST_BLAST_FILE"
			echo blast_output="$blast_output" >> "$POST_BLAST_FILE"
			echo BLAST_INPUT="${BLAST_INPUT}" >> "$POST_BLAST_FILE"
			echo "###################################################################################################" >> "$POST_BLAST_FILE"
#			cat /home/mbonteam/Reiko_scripts/New_Post_Blast.sh  >> "$POST_BLAST_FILE"
			cat /home/mbonteam/Reiko_scripts/Post_Blast_megan6_Reiko.sh  >> "$POST_BLAST_FILE"
	else
		echo
		echo 'BLAST failed: the output file is empty or absent.'
		echo 'File should be:' "${blast_output}"
		echo
		blast_fail="YES"
	fi # if blast_output


done


blast_cnt=$(grep -o  'DUP_' "$blast_output"  | wc -l)

	if [ "$PERFORM_CLEANUP" = "YES" ]; then
		echo $(date +%H:%M) "Compressing fasta, fastq, and xml files..."
		find "${ANALYSIS_DIR}" -type f -name '*.fasta' -exec ${ZIPPER} "{}" \;
		find "${ANALYSIS_DIR}" -type f -name '*.fastq' -exec ${ZIPPER} "{}" \;
		find "${ANALYSIS_DIR}" -type f -name '*.xml' -exec ${ZIPPER} "{}" \;
	# to conserve disk space the following files are removed.  For debugging you may want to comment one of these files.  	
		 #rm "${ANALYSIS_DIR}"/all_lib/nosingle.txt
		 #rm "${ANALYSIS_DIR}"/all_lib/1_demult_concat.fasta.derep
		 #rm "${ANALYSIS_DIR}"/all_lib/1_demult_concat.fasta.gz
		 #rm -r "${ANALYSIS_DIR}"/all_lib/dup_temp
		echo $(date +%H:%M) "Cleanup performed."
	else
		echo $(date +%H:%M) "Cleanup not performed."
	fi

	# create output files with standard_names
	cp "${DIR}"/OTUs_swarm/OTUs.fasta.gz "${DIR}"/"${STANDARD_PREFIX}"_"${DB_NAME}"_OTUs.fasta.gz

	if [ "$blast_fail" = "NO" ]; then
		echo -e '\n'$(date +%H:%M)'\t Pipeline finished! Why not treat yourself to a...\n'
		echo
		echo -e '\t~~~ MAI TAI ~~~'
		echo -e '\t2 oz\taged rum'
		echo -e '\t0.75 oz\tfresh squeezed lime juice'
		echo -e '\t0.5 oz\torgeat'
		echo -e '\t0.5 oz\ttriple sec'
		echo -e '\t0.25 oz\tsimple syrup'
		echo -e '\tShake, strain, and enjoy!' '\xf0\x9f\x8d\xb9\x0a''\n'


		echo '******************** TAXONOMIC ASSIGNMENT **********************'
		echo '*                                                              *'
		echo '*   Run Post_BLAST_processing file from your script directory  *'
		echo '*        bash  Post_BLAST_processing_'${START_TIME}'.sh        *'
		echo '*                                                              *'
		echo '*********************** MEGAN & BIOM ***************************'

fi


echo ''
echo ''
echo 'OTU_table count=' "$otu_cnt"
echo 'OTU_fasta count=' "$otu_fasta_cnt"
echo 'BLAST count=' "$blast_cnt"

FINISH_TIME=$(date +%Y%m%d_%H%M)

if [ "$NOTIFY_EMAIL" = "YES" ]; then
	echo 'Pipeline finished! Started at' $START_TIME 'and finished at' $FINISH_TIME | mail -s "banzai is finished" "${EMAIL_ADDRESS}"
else
	echo 'Pipeline finished! Started at' $START_TIME 'and finished at' $FINISH_TIME
fi
echo 'Data are in ' "${ANALYSIS_DIR}"
# remove some of the repetitive lines, read percents from kmers, fastq etc.  makes it difficult to read logfile.
# grep -v flag means output everything except pattern, -f means pattern is in text file, i.e. grep_patterns.txt
mv "${ANALYSIS_DIR}"/logfile.txt "${ANALYSIS_DIR}"/logfile_orig.txt 
grep -v -f /home/mbonteam/MBARI/reiko/scripts/grep_patterns.txt "${ANALYSIS_DIR}"/logfile_orig.txt > "${ANALYSIS_DIR}"/logfile.txt
# Quick scan of logfile2.txt for errors, error line and 3 lines before, -B 3
grep -n -B 3 "error" "${ANALYSIS_DIR}"/logfile.txt >"${ANALYSIS_DIR}"/errors.txt
